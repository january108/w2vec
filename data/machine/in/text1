機械学習 [はてなブックマークで表示] [はてなブックマークで表示]
機械学習および
データマイニング
Kernel Machine.svg
問題[表示]
教師あり学習（分類 • 回帰）
[表示]
クラスタリング[表示]
次元削減（英語版）[表示]
構造化予測（英語版）[表示]
異常検知[表示]
ニューラルネットワーク[表示]
強化学習[表示]
理論[表示]
議論の場[表示]
表 話 編 歴
機械学習（きかいがくしゅう、英: machine learning）とは、人工知能における研究課題の一つで、人間が自然に行っている学習能力と同様の機能をコンピュータで実現しようとする技術・手法のことである。
目次  [非表示] 
1	概要
1.1	定義
1.2	一般化
1.3	人間との相互作用
2	データマイニングとの関係
3	アルゴリズムの分類
4	理論
5	技法
6	応用分野
7	ソフトウェア
8	学術誌と国際学会
9	脚注
10	参考文献
11	関連項目
12	外部リンク
概要[編集]
センサやデータベースなどから、ある程度の数のサンプルデータ集合を入力して解析を行い、そのデータから有用な規則、ルール、知識表現、判断基準などを抽出し、アルゴリズムを発展させる。なお、データ集合を解析するので、統計学との関連が深い。
そのアルゴリズムは、第一にそのデータが生成した潜在的機構の特徴を捉え、複雑な関係を識別（すなわち定量化）する。第二にその識別したパターンを用いて、新たなデータについて予測を行う。データは、観測された変数群のとりうる関係の具体例と見ることができる。一方、アルゴリズムは、機械学習者として観測されたデータの部分（訓練例などと呼ぶ）を学習することで、データに潜在する確率分布の特徴を捉え、学習によって得た知識を用いて、新たな入力データについて知的な決定を行う[1]。
1つの根本的な課題は、観測例に全てのとりうる挙動例を示すあらゆる入力を含めるのは（多くの実用的な関心事の場合）大きすぎて現実的でないという点である。したがって、学習者は与えられた例を一般化して、新たなデータ入力から有用な出力を生成しなければならない[1]。
光学文字認識では、印刷された活字を事前の例に基づいて自動認識する。これは典型的な機械学習の応用例である[1]。
機械学習は検索エンジン、医療診断、スパムメールの検出、金融市場の予測、DNA配列の分類、音声認識や文字認識などのパターン認識、ゲーム戦略、ロボット、など幅広い分野で用いられている。応用分野の特性に応じて学習手法も適切に選択する必要があり、様々な手法が提案されている[2]。これらの手法は、テストデータにおいての検出・予測性能において評価されることがある。大量のデータから従来にない知見を得るというビッグデータの時代では、特にその応用に期待が集まっている[3]。
定義[編集]
1959年、アーサー・サミュエルは、機械学習を「明示的にプログラムしなくても学習する能力をコンピュータに与える研究分野」だとした[4]。
トム・M・ミッチェル（英語版）は、よく引用されるさらに厳格な定義として「コンピュータプログラムが、ある種のタスクTと評価尺度Pにおいて、経験Eから学習するとは、タスクTにおけるその性能をPによって評価した際に、経験Eによってそれが改善されている場合である」とした[5]。
一般化[編集]
この文脈における一般化とは、学習用データセットを使って訓練した後に、未知の例について正確に判断できるアルゴリズムの能力をいう。学習者の最も重要な目的は、経験から一般化することである[6]。訓練例は、一般に未知の確率分布に従っており、学習者はそこから新たな例について有用な予測を生み出す何か一般的なもの、その分布に関する何かを引き出す必要がある。
人間との相互作用[編集]
機械学習システムによっては、人間の直観によるデータ解析の必要性を排除しようとしているが、人間と機械の協調的相互作用を取り入れたものもある。しかし、そもそもシステムのデータ表現方法やデータの特徴を探る機構は、人間が設計したものであり、人間の直観を完全に排除することはできない。
データマイニングとの関係[編集]
機械学習とデータマイニングは交差する部分が大きく、技法も同じなので混同されることが多いが、次のように定義できる。
機械学習の目的は、訓練データから学んだ「既知」の特徴に基づく予測である。
データマイニングの目的は、それまで「未知」だったデータの特徴を発見することである。
この2つは、さまざまな面でオーバーラップしている。データマイニングは、機械学習の技法を使うが、その目的は若干異なることが多い。一方、機械学習もデータマイニングの技法を「教師なし学習」として、あるいは学習者の正確性を向上させる前処理として用いる。2つの研究領域は、ECML PKDD という例外はあるが、基本的に学会も学術誌も別々である。それらの間の混同の最大の原因は、それらの基本的前提に由来する。機械学習では、既知の知識を再生成できるかどうかで性能を評価するが、データマイニングではそれまで「未知」だった知識を発見することが重視される。したがって、既知の知識によって評価するなら「教師なしの技法」よりも「教師ありの技法」の方が容易に優れた結果を示すことができる。しかし、典型的なデータマイニングでは、訓練データが用意できないので、「教師ありの技法」を採用することができない。
アルゴリズムの分類[編集]
機械学習のアルゴリズムは、要求される結果により以下のように分類される。
教師あり学習
入力とそれに対応すべき出力（人間の専門家が訓練例にラベル付けすることで提供されることが多いのでラベルとも呼ばれる）を写像する関数を生成する。例えば、分類問題では入力ベクトルと出力に対応する分類で示される例を与えられ、それらを写像する関数を近似的に求める。
教師なし学習
入力のみ（ラベルなしの例）からモデルを構築する。データマイニングも参照。
半教師あり学習（英語版）
ラベルありの例とラベルなしの例をどちらも扱えるようにしたもので、それによって近似関数または分類器を生成する。
強化学習
周囲の環境を観測することでどう行動すべきかを学習する。行動によって必ず環境に影響を及ぼし、環境から報酬という形でフィードバックを得ることで学習アルゴリズムのガイドとする。例えばQ学習がある。
トランスダクション（英語版）（トランスダクティブ推論）
観測された具体的な（訓練）例から具体的かつ固定の（テスト）例の新たな出力を予測しようとする。
マルチタスク学習（英語版）
関連する複数の問題について同時に学習させ、主要な問題の予測精度を向上させる。
理論[編集]
機械学習アルゴリズムとその性能についての分析は、理論計算機科学の一分野であり、計算論的学習理論（英語版）と呼ばれている。訓練例は有限であるのに対して、未来は不確かであるため、学習理論は一般にアルゴリズムの性能を保証できない。その代わりに、性能の確率的範囲を与える。 Wassily Hoeffding（英語版）によるヘフディングの不等式（英語版）など統計的学習理論という表現もある。[7]
それに加えて、学習の時間複雑性と実現可能性についても研究している。計算論的学習理論では、多項式時間で終了する計算を実現可能とみなす。
機械学習と統計学は、多くの点で似ているが、使用する用語は異なる。
技法[編集]
決定木学習
決定木を予測モデル（英語版）として使用した学習であり、アイテムについての観測をそのアイテムの目標値についての結論とマッピングする。具体例としてID3やRandom forestがある。
相関ルール学習（英語版）
大規模データベースにおける変数間の興味深い関係を発見するための技法。
ニューラルネットワーク (NN)
人工ニューラルネットワーク (ANN) とも呼ばれ、生物の神経ネットワークの構造と機能を模倣するという観点から生まれた学習アルゴリズムである。人工神経を相互接続したもので計算を構造化し、コネクショニズム的計算技法で情報を処理する。現代的ニューラルネットワークは非線形な統計的データモデリングツールである。入力と出力の間の複雑な関係をモデル化するのに使われ、データのパターン認識や観測された変数間の未知の同時分布における統計的構造を捉えるなどの用途がある。
遺伝的プログラミング (GP)
生物の進化を模倣した進化的アルゴリズムに基づく技法であり、ユーザーが定義したタスクを実行するプログラムを探索する。遺伝的アルゴリズムを拡張・特化させたものである。所定のタスクを実行する能力によって適応度地形を決定し、それによってコンピュータプログラムを最適化させていく機械学習技法である。
帰納論理プログラミング（英語版） (ILP)
例、背景知識、仮説を一様な表現とし、論理プログラミングを使って学習を規則化する技法である。既知の背景知識と例の集合をコード化して事実の論理データベースとし、全てのポジティブな例を含み、ネガティブな例を全く含まない仮説的論理プログラムを生成する。
サポートベクターマシン (SVM)
分類や回帰に使われる一連の教師あり学習技法である。訓練例のラベルは二値分類（2つに分類される）であり、訓練アルゴリズムによってモデルを構築し、新たな例がどちらに分類されるかを予測する。
クラスタリング
クラスタリングは、観測された例をクラスタと呼ばれる部分集合に振り分けるもので、振り分けは事前に指示された基準に従って行う。クラスタリングはデータの構造についての仮説（基準）の立て方によって結果が異なる。仮説は「類似尺度」で定義され、「内部コンパクト性」（同一クラスタ内のメンバー間の類似性）や異なるクラスタ間の距離によって評価される。「推定密度」や「グラフ接続性」に基づく技法もある。クラスタリングは教師なし学習技法であり、統計的データ解析でよく使われる。
ベイジアンネットワーク
確率変数群とそれらの条件付き独立性（英語版）を有向非巡回グラフ (DAG) で表した確率論的グラフィカルモデルである。例えば、病気と症状の関係を確率的に表すことができる。そのネットワークに症状を入力すれば、考えられる病気の一覧を確率付きで出力できる。これを使って推論と学習を行う効率的アルゴリズムが存在する。
強化学習
「エージェント」が「環境」の中でどのような「行動」をとるべきかを、何らかの長期的「報酬」を最大化するよう決定する。環境の「状態」からエージェントの行動への写像を行う「方針」を求めるのが強化学習アルゴリズムである。正しい入出力例は与えられないし、最適でない行動が明示的に訂正されることもないので、教師あり学習とは異なる。
表現学習（英語版）
教師なし学習アルゴリズムの一部は、訓練中に提供された入力のよりよい表現を発見しようとする。古典的な例として主成分分析やクラスタ分析がある。入力の持つ情報は保持したまま、分類や予測の前に入力をより便利な表現に変換するアルゴリズムもある。その際に入力データが従っている未知の確率分布から入力を再建できるようにするが、その確率分布においては信じがたい例も忠実に再現する必要はない。例えば多様体学習（英語版）アルゴリズムは、何らかの制約下で入力の次元を低く変換して表現する。スパースコーディング（英語版）アルゴリズムでは、入力が疎ら（ゼロが多い）という制約下で同様の表現の変換を行う。ニューラルネットワークの深層学習は複数レベルの表現または特徴の階層を発見するもので、低いレベルで抽出した特徴から高いレベルの抽象化した特徴までを求める。知的機械は、観測されたデータを説明する偏差の潜在的要因を解きほぐす表現を学習するものだという主張もある[8]。
応用分野[編集]
機械学習には以下のような応用分野がある。
機械知覚（英語版）
コンピュータビジョン
自然言語処理
統語的パターン認識（英語版）
検索エンジン
（人工知能による）診断
バイオインフォマティクス
ブレイン・マシン・インタフェース
ケモインフォマティクス
クレジットカード詐欺 (credit card fraud) の検出
証券市場分析
塩基配列の分類
シーケンスマイニング（英語版）
音声認識と手書き文字認識
物体認識 （コンピュータビジョン）
ゲームプレイ
ソフトウェア工学
適応型ウェブサイト（英語版）
移動ロボット（英語版）
金融工学
構造ヘルスモニタリング（英語版）
感情分析（英語版）（意見マイニング）
感情コンピューティング（英語版）
情報検索
レコメンダシステム
2006年、オンラインDVDレンタル会社ネットフリックスは、同社のレコメンダシステムより10%以上高性能な（ユーザーの好みをより正確に予測する）プログラムを捜す競技会 Netflix Prize を開催した。この競技会は数年かけて行われ、AT&T Labs のチームが "Pragmatic Chaos" という機械学習プログラムで2009年に優勝し100万ドルを獲得した[9]。
ソフトウェア[編集]
各種機械学習アルゴリズムを備えたソフトウェアスイートとして、SAS・RapidMiner・LIONsolver・KNIME・Weka・ODM・Shogun toolbox・Orange・Apache Mahout・scikit-learn・mlpy・MCMLL・OpenCV・XGBoost・Jubatus などがある。
学術誌と国際学会[編集]
Machine Learning（学術誌）
Journal of Machine Learning Research（学術誌）
Neural Computation（学術誌）
International Conference on Machine Learning (ICML)（国際学会）
Neural Information Processing Systems (NIPS)（国際学会）
脚注[編集]
^ a b c Wernick, Yang, Brankov, Yourganov and Strother, Machine Learning in Medical Imaging, IEEE Signal Processing Magazine, vol. 27, no. 4, July 2010, pp. 25-38
^ それらの手法は、Machine Learning や IEEE Transactions on Pattern Analysis and Machine Intelligence などの学術雑誌などで発表されることが多い。
^ もう一度「ビッグデータ」を考える
^ http://holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr.html
^ Mitchell, T. (1997). Machine Learning, McGraw Hill. ISBN 0-07-042807-7, p.2.
^ Christopher M. Bishop (2006) Pattern Recognition and Machine Learning, Springer ISBN 0-387-31073-8.
^ 統計的学習理論, 金森敬文, 機械学習プロフェッショナルシリーズ, 講談社, 2015, ISBN 9784061529052
^ Yoshua Bengio (2009). Learning Deep Architectures for AI. Now Publishers Inc.. p. 1–3. ISBN 978-1-60198-294-0.
^ "BelKor Home Page" research.att.com
参考文献[編集]
Thomas Mitchell "Machine Learning" McGraw-Hill (1997) ISBN 978-0071154673 (入門用の教科書) →サポートページ
Christopher M. Bishop "Pattern Recognition And Machine Learning" Springer-Verlag (2006) ISBN 978-0387310732 (中上級の教科書) →サポートページ（ここから、第8章 "Graphical Models" をpdf形式で入手可能）
日本語版「パターン認識と機械学習 - ベイズ理論による統計的予測」シュプリンガージャパン (2007-2008) 上巻：ISBN 978-4431100133 下巻：ISBN 978-4431100317 →日本語版サポートページ
Trevor Hastie, Robert Tibshirani, and Jerome H. Friedman "The Elements of Statistical Learning: Data Mining, Inference, and Prediction" Springer-Verlag (2001) ISBN 978-0387952840 (高度な内容も含む．数理・統計系の手法が中心) →サポートページ（ここから、全章をpdf形式で入手可能）
David MacKay "Information Theory, Inference, and Learning Algorithms" (2003) (ベイズ推論を中心に、情報理論と機械学習を包括的にカバーした教科書) →著者ページ（ここから全文をPDF形式で入手可能）
Sergios Theodoridis, Konstantinos Koutroumbas (2009) "Pattern Recognition", 4th Edition, Academic Press, ISBN 978-1-59749-272-0.
Ethem Alpaydın (2004) Introduction to Machine Learning (Adaptive Computation and Machine Learning), MIT Press, ISBN 0-262-01211-1
Bing Liu (2007), Web Data Mining: Exploring Hyperlinks, Contents and Usage Data. Springer, ISBN 3-540-37881-2
Toby Segaran (2007), Programming Collective Intelligence, O'Reilly, ISBN 0-596-52932-5
Ray Solomonoff, "An Inductive Inference Machine" A privately circulated report from the 1956 Dartmouth Summer Research Conference on AI.
Ray Solomonoff, An Inductive Inference Machine, IRE Convention Record, Section on Information Theory, Part 2, pp., 56-62, 1957.
Ryszard S. Michalski, Jaime G. Carbonell, Tom M. Mitchell (1983), Machine Learning: An Artificial Intelligence Approach, Tioga Publishing Company, ISBN 0-935382-05-4.
Ryszard S. Michalski, Jaime G. Carbonell, Tom M. Mitchell (1986), Machine Learning: An Artificial Intelligence Approach, Volume II, Morgan Kaufmann, ISBN 0-934613-00-1.
Yves Kodratoff, Ryszard S. Michalski (1990), Machine Learning: An Artificial Intelligence Approach, Volume III, Morgan Kaufmann, ISBN 1-55860-119-8.
Ryszard S. Michalski, George Tecuci (1994), Machine Learning: A Multistrategy Approach, Volume IV, Morgan Kaufmann, ISBN 1-55860-251-8.
Bishop, C.M. (1995). Neural Networks for Pattern Recognition, Oxford University Press. ISBN 0-19-853864-2.
Richard O. Duda, Peter E. Hart, David G. Stork (2001) Pattern classification (2nd edition), Wiley, New York, ISBN 0-471-05669-3.
Huang T.-M., Kecman V., Kopriva I. (2006), Kernel Based Algorithms for Mining Huge Data Sets, Supervised, Semi-supervised, and Unsupervised Learning, Springer-Verlag, Berlin, Heidelberg, 260 pp. 96 illus., Hardcover, ISBN 3-540-31681-7.
KECMAN Vojislav (2001), Learning and Soft Computing, Support Vector Machines, Neural Networks and Fuzzy Logic Models, The MIT Press, Cambridge, MA, 608 pp., 268 illus., ISBN 0-262-11255-8.
Ian H. Witten and Eibe Frank (2011). Data Mining: Practical machine learning tools and techniques Morgan Kaufmann, 664pp., ISBN 978-0123748560.
Sholom Weiss and Casimir Kulikowski (1991). Computer Systems That Learn, Morgan Kaufmann. ISBN 1-55860-065-5.
Mierswa, Ingo and Wurst, Michael and Klinkenberg, Ralf and Scholz, Martin and Euler, Timm: YALE: Rapid Prototyping for Complex Data Mining Tasks, in Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-06), 2006.
Vladimir Vapnik (1998). Statistical Learning Theory. Wiley-Interscience, ISBN 0-471-03003-1.
ピーター フラッハ, 竹村 彰通 (監訳）、「機械学習 ─データを読み解くアルゴリズムの技法─」、朝倉書店、ISBN978-4254122183　（2017年4月5日）。
関連項目[編集]
自動推論
計算知能
計算論的神経科学
認知科学
認知モデル
データマイニング
パターン認識
カーネル法
エンタープライズサーチ
次元の呪い
言語獲得
ワトソン (コンピュータ)
外部リンク[編集]
電子情報通信学会 情報論的学習理論と機械学習 (IBISML) 研究会
朱鷺の杜Wiki 機械学習・データマイニングについてのWiki
[隠す]
表 話 編 歴
統計学
標本調査	
標本 母集団 無作為抽出 層化抽出法
要約統計量	
連続データ	
位置	
平均 算術 幾何 調和  中央値 分位数  最頻値 階級値
分散	
範囲 標準偏差 変動係数 百分率
モーメント	
分散 歪度 尖度
カテゴリデータ	
頻度 分割表
統計的推測	
仮説検定	
帰無仮説 対立仮説 有意 棄却 ノンパラメトリック手法 スチューデントのt検定 ウェルチのt検定 カイ二乗検定 イェイツのカイ二乗検定 累積カイ二乗検定 F検定 G検定 マン・ホイットニーのU検定 Z検定 フィッシャーの正確確率検定 二項検定 尤度比検定 マンテル検定 コクラン・マンテル・ヘンツェルの統計量 ウィルコクソンの符号順位検定 アンダーソン–ダーリング検定 カイパー検定 ジャック–ベラ検定 シャピロ–ウィルク検定 コルモゴロフ–スミルノフ検定 分散分析 共分散分析
区間推定	
信頼区間 予測区間
その他	
最尤推定 最大事後確率 ベイズ推定 尤度関数 カーネル密度推定 最小距離推定 メタアナリシス
生存時間分析	
生存時間関数 カプラン＝マイヤー推定量 ログランク検定 故障率 比例ハザードモデル
相関	
交絡変数 ピアソンの積率相関係数 順位相関 スピアマンの順位相関係数 ケンドールの順位相関係数
モデル	
一般線形モデル 一般化線形モデル 混合モデル 一般化線形混合モデル
回帰	
線形	
線形回帰 リッジ回帰 Lasso エラスティックネット
非線形	
k近傍法 回帰木 ランダムフォレスト ニューラルネットワーク サポートベクター回帰 射影追跡回帰
分類	
線形	
線形判別分析 ロジスティック回帰 単純ベイズ分類器 単純パーセプトロン 線形サポートベクターマシン
二次	
二次判別分析
非線形	
k近傍法 決定木 ランダムフォレスト ニューラルネットワーク サポートベクターマシン ベイジアンネットワーク 隠れマルコフモデル
その他	
二項分類 多クラス分類 第一種過誤と第二種過誤
教師なし学習	
クラスタリング	
k平均法 k-means++法
その他	
主成分分析 独立成分分析 自己組織化写像
統計図表	
棒グラフ バイプロット 箱ひげ図 管理図 森林プロット ヒストグラム 円グラフ Q-Q プロット ランチャート 散布図 幹葉図 バイオリン図
歴史	
統計学歴史 統計学の創始者 確率論と統計学の歩み
応用	
社会統計学 生物統計学 統計力学 計量経済学 機械学習 実験計画法
出版物	
統計学に関する学術誌一覧 重要な出版物
カテゴリ
典拠管理	
GND: 4193754-5 NDL: 001210569
カテゴリ: 機械学習人工知能学習サイバネティックス
