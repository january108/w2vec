GoogleのCloud PlatformがGPUをサポート3か月前にGoogleは、2017年の早い時期に、機械学習などの特殊なワークロードためにハイエンドのグラフィクスプロセシングユニット(graphics processing unit, GPU)のサポートを開始する、と発表した。2017年の早い時期とは今のことだから、Googleは言葉に違（たが）わず今日から、Google Cloud Platform上でGPUを使えるようにした。予想通りそれはNvidiaのTesla K80で、ユーザー（デベロッパー）はひとつのCompute Engineマシンで最大8つを動かすことができる。GPUベースの仮想マシンを使えるのは当面、三つのデータセンター、us-east1, asia-east1, そしてeurope-west1だけだ。ひとつのK80コアに2496のストリームプロセッサーと12GBのGDDR5メモリがある（K80ボードには2つのコアと24GBのRAMがある）。複雑なシミュレーションを動かしたり、TensorFlow, Torch, MXNet, Caffeeなどのディープラーニングフレームワークを使っているときには、計算力はどれだけあっても過剰ではない。GoogleがこれらのGPUインスタンスでねらっているのも、ハイエンドマシンのクラスタを常時動かして機械学習のフレームワークを駆動しているようなデベロッパーだ。このGoogle Cloud GPUは、GoogleのCloud Machine Learningサービスおよび同社のさまざまなデータベースとストレージのプラットホームに統合される。GPUの利用料金単価はアメリカでは1時間70セント、ヨーロッパとアジアのデータセンターでは77セントだ。時間単価としてはお安くないが、Tesla K80の2コア/24GB RAMのアクセラレータは、たちまち数千ドルの節約を稼ぎだしてくれるだろう。この発表から数週間後にGoogleはサンフランシスコで、Cloud NEXTカンファレンスを開催する。そこではおそらく、同社の機械学習をもっと多くのデベロッパーに使ってもらうための企画が、発表されるだろう。〔参考記事: AWSのGPUインスタンス〕