TensorFlowライブラリによる機械学習モデルの、本番アプリケーションへの実装を助けるAPI集TensorFlow ServingをGoogleがリリースGoogleが今日（米国時間2/16）、TensorFlow Servingをローンチした。これは、デベロッパーが着想した機械学習モデルの、プロダクション環境における実装を助けるオープンソースのプロジェクトだ。TensorFlow Servingはその名のとおり、Googleの機械学習ライブラリTensorFlowに向けて最適化されているが、しかし同社によると、そのほかのモデルやデータをサポートするよう拡張もできる。TensorFlowのようなプロジェクトがあれば、機械学習のアルゴリズムを作ったり、それらを特定のタイプのデータ入力に対して訓練することが容易にできるようになるが、TensorFlow Servingはこれらのモデルをデベロッパーのプロダクション環境で（本格的なアプリケーションで）実際に使えるようにする。デベロッパーはTensorFlowを使って自分のモデルを訓練し、それからTensorFlow ServingのAPIを使ってクライアントからの入力に反応できるようにする。Googleによると、TensorFlow Servingは、マシンのGPUリソースを使って処理を高速化できる。ただしGoogleによると、このようなシステムがあるからといって、機械学習モデルのプロダクションへの実装工程が速くなるわけではない。デベロッパーはしかし、アーキテクチャやAPIの安定性を維持しながら、ほかのアルゴリズムやモデルを試すことができる。さらにまた、デベロッパーがそのモデルをアップデートしたり、出力が新しい入力データによって変わったりしても、アーキテクチャの安定性は維持される。TensorFlow Servingは、Go言語ではなくC++で書かれている。そのソフトウェアはパフォーマンスの向上のために最適化されており、同社によると、16コアのXeon機上で1コアあたり毎秒10万以上のクェリを処理できる。TensorFlow Servingのコードとチュートリアル類は、Apache 2.0のライセンスによりGitHubで入手できる。