FaceAppが人種差別的AIを構築したことを謝罪すべてのアルゴリズムバイアスがこれと同じくらい簡単に見つけられれば良いのに。フォトリアリスティックなやり方で、自撮り写真の編集をニューラルネットワークで行う写真編集アプリFaceAppは、人種差別的（racist）アルゴリズムを提供したことを謝罪した。このアプリは、ユーザーが自撮り写真や、撮影済みの顔の写真をアップロードして、その見かけを微妙にあるいは大胆に変えるフィルターの適用を行わせるものだ。風貌を変える効果の中には、老化や異性化なども含まれている。問題は、アプリには”hotness”（ホットにする）フィルターも含まれていたことで、このフィルターが人種差別主義者だったのだ。あるユーザーが指摘したように、このフィルターは議論の余地がある「美肌化」効果を出すために、肌の色調を明るく調整するのだ。フィルターの適用前後の効果は、上のオバマ大統領（当時）の写真で見ることができる。人種差別的なアルゴリズムを謝罪する電子メールによる声明で、FaceAppの創業者兼CEOのYaroslav Goncharovは次のように語っている「この疑いようもない深刻な問題に対して深くお詫び申し上げます。これは、意図された振る舞いではなく、訓練セットの偏りによって引き起こされた、基盤を構成するニューラルネットワークによる不幸な副作用でした。この問題を緩和するために、私たちはフィルタの改名を行い、その効果に対する肯定的な含意を排除しました。また程なくリリースされる完全な修正にも取り組んでいます」。先にGuardianが指摘したように、このアプリはここ数週間で爆発的な人気を得た。恐らくこのことが、フィルターが問題を抱えていることをFaceAppが認識することを助けたのだろう。FaceAppは一時的に不快なフィルタの名前を “hotness”から “spark”に変更したが、非人種差別的な代替品の出荷準備が整うまではアプリから完全に削除しておいたほうがより賢明だったかもしれない。おそらく彼らはアプリのクチコミパワーの高まりへの対応に手一杯なのだろう（明らかに毎日70万人のユーザーが増え続けている）。FaceAppのエフェクトを支える基盤となるAI技術には、GoogleのTensorFlowなどのオープンソースライブラリのコードも使われているが、”hotness”フィルターのトレーニングに用いられたデータセットは彼ら独自のもので、公開されているデータセットではないということを、Goncharovは私たちに明言した。そのため何処に責任があるかについては議論の余地はない。率直に言って、アルゴリズム内に埋め込まれたバイアスによるリスクの（ビジュアルな）例として、これ以上わかりやすいものを探すのは難しいだろう。機械学習モデルは、与えられたデータと同じ程度にしか良くはならない。そしてFaceAppの場合には、モスクワに拠点を置くチームによって行われたトレーニングデータには、明らかに十分な多様性が欠落していたのだ。少なくとも、 潜在的なアルゴリズムバイアスの目に見えない問題を、このような視覚的にインパクトのある方法で提示してくれたという点では、彼らには感謝することができるだろう。AIがますます多くのシステムの制御を手渡されるようになれば、その審問を完全に行うためにアルゴリズムの説明責任への圧力が高まるし、人間のバイアスが私たちのマシンに埋め込まれることを避けるための堅牢なシステムの開発の必要性も高まる。自律技術とは「人間の欠陥から自由になれる」ものではない、もし人間の欠陥から自由になれると主張しようとする開発者がいるならば、それは嘘を売ろうとしているのだ。[ 原文へ ]
（翻訳：Sako） 